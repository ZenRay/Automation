#coding:utf-8
"""
MaxComputeç»´åº¦è¡¨ETL DAG
ç”¨äºæ‰§è¡Œç»´åº¦è¡¨çš„ETLä»»åŠ¡ï¼ŒåŒ…æ‹¬å•†å“å±æ€§ã€åº—é“ºæ—¥åº¦ã€å†å²äº¤æ˜“ç­‰ç»´åº¦è¡¨
"""

import sys
import os
import logging
import json
from typing import Dict, Any
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.hooks.base import BaseHook
from datetime import datetime, timedelta

# æ·»åŠ etl_sentenceæ¨¡å—è·¯å¾„åˆ°Pythonæœç´¢è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
etl_sentence_path = os.path.join(current_dir, 'etl_sentence')
if etl_sentence_path not in sys.path:
    sys.path.insert(0, etl_sentence_path)
    sys.path.insert(0, current_dir)

logger = logging.getLogger("dispatcher.dags")

# å¯¼å…¥etl_sentenceæ¨¡å—ä¸­çš„SQLè¯­å¥
from etl_sentence.maxcompute_sql.dimention import (
    dim_changsha_goods_property_sentence, 
    dim_changsha_store_daily_full,
    dim_store_history_trade_temp
)

# å¯¼å…¥automationæ¨¡å—
from automation.client import MaxComputerClient
from automation import hints


def build_maxcompute_config(conn_id: str = 'maxcompute_dev') -> Dict[str, Any]:
    """
    æ„å»ºMaxComputeè¿æ¥é…ç½®
    
    Args:
        conn_id: è¿æ¥ID
        
    Returns:
        åŒ…å«è¿æ¥é…ç½®çš„å­—å…¸
        
    Raises:
        ValueError: é…ç½®ä¸å®Œæ•´æ—¶æŠ›å‡º
        Exception: å…¶ä»–é”™è¯¯æ—¶æŠ›å‡º
    """
    try:
        # è·å–è¿æ¥é…ç½®
        conn = BaseHook.get_connection(conn_id)
        
        # è®°å½•è¿æ¥åŸºæœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
        logger.info(f"è¿æ¥ç±»å‹: {conn.conn_type}")
        logger.info(f"è¿æ¥ID: {conn.conn_id}")
        
        # æ ¹æ®è¿æ¥ç±»å‹æ„å»ºé…ç½®
        if conn.conn_type == 'maxcompute':
            # MaxComputeè¿æ¥ç±»å‹ï¼šä»Extraå­—æ®µè§£æé…ç½®
            extra = json.loads(conn._extra) if conn._extra else {}
            conf = {
                "access_id": extra.get('access_key_id'),
                "secret_access_key": extra.get('access_key_secret'),
                "project": extra.get('project'),
                "endpoint": extra.get('endpoint')
            }
        elif conn.conn_type == 'Generic':
            # Genericè¿æ¥ç±»å‹ï¼šä»Extraå­—æ®µè§£æé…ç½®
            extra = json.loads(conn.extra) if conn.extra else {}
            conf = {
                "access_id": conn.login,
                "secret_access_key": conn.password,
                "project": extra.get('project'),
                "endpoint": extra.get('endpoint', conn.host)
            }
        else:
            # å…¶ä»–è¿æ¥ç±»å‹ï¼šä½¿ç”¨æ ‡å‡†å­—æ®µ
            conf = {
                "access_id": conn.login,
                "secret_access_key": conn.password,
                "project": conn.schema,
                "endpoint": conn.host
            }
        
        # éªŒè¯é…ç½®å®Œæ•´æ€§
        required_fields = ['access_id', 'secret_access_key', 'project', 'endpoint']
        missing_fields = [field for field in required_fields if not conf.get(field)]
        
        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„é…ç½®å­—æ®µ: {', '.join(missing_fields)}")
        
        # ç¡®ä¿endpointåŒ…å«åè®®
        if conf['endpoint'] and not conf['endpoint'].startswith(('http://', 'https://')):
            conf['endpoint'] = f"https://{conf['endpoint']}"
        
        logger.info(f"MaxComputeé…ç½®æ„å»ºæˆåŠŸ: project={conf['project']}, endpoint={conf['endpoint']}")
        return conf
        
    except Exception as e:
        logger.error(f"æ„å»ºMaxComputeé…ç½®å¤±è´¥: {str(e)}")
        raise


# å…¨å±€MaxComputeå®¢æˆ·ç«¯
_mc_client = None


def get_maxcompute_client() -> MaxComputerClient:
    """
    è·å–MaxComputeå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        MaxComputeå®¢æˆ·ç«¯å®ä¾‹
    """
    global _mc_client
    if _mc_client is None:
        conf = build_maxcompute_config()
        _mc_client = MaxComputerClient(**conf)
        logger.info("MaxComputeå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    return _mc_client


def execute_sql_task(sql_sentence: str, task_name: str, hints: Dict[str, Any] = None) -> bool:
    """
    æ‰§è¡Œå•ä¸ªSQLä»»åŠ¡
    
    Args:
        sql_sentence: SQLè¯­å¥
        task_name: ä»»åŠ¡åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        hints: æ‰§è¡Œæç¤ºå‚æ•°
        
    Returns:
        æ‰§è¡ŒæˆåŠŸè¿”å›True
        
    Raises:
        Exception: æ‰§è¡Œå¤±è´¥æ—¶æŠ›å‡º
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œ {task_name}...")
        
        # è·å–å®¢æˆ·ç«¯å¹¶æ‰§è¡ŒSQL
        client = get_maxcompute_client()
        client.execute_sql(sql_sentence, hints=hints or {})
        
        logger.info(f"âœ… {task_name} æ‰§è¡Œå®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {task_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise


def execute_dimension_etl():
    """æ‰§è¡Œç»´åº¦è¡¨ETLä»»åŠ¡ï¼ˆæŒ‰ä¾èµ–é¡ºåºï¼‰"""
    try:
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒMaxComputeç»´åº¦è¡¨ETLä»»åŠ¡...")
        
        # æŒ‰ä¾èµ–é¡ºåºæ‰§è¡ŒETLä»»åŠ¡
        etl_tasks = [
            {
                'sql': dim_changsha_goods_property_sentence,
                'name': 'å•†å“å±æ€§ç»´åº¦è¡¨ETL',
                'description': 'æ„å»ºå•†å“å±æ€§ç»´åº¦è¡¨ï¼Œä¸ºåç»­åˆ†ææä¾›åŸºç¡€æ•°æ®'
            },
            {
                'sql': dim_changsha_store_daily_full,
                'name': 'åº—é“ºæ—¥åº¦ç»´åº¦è¡¨ETL',
                'description': 'æ„å»ºåº—é“ºæ—¥åº¦ç»´åº¦è¡¨ï¼Œä¾èµ–å•†å“å±æ€§è¡¨å®Œæˆ'
            },
            {
                'sql': dim_store_history_trade_temp,
                'name': 'å†å²äº¤æ˜“ç»´åº¦è¡¨ETL',
                'description': 'æ„å»ºå†å²äº¤æ˜“ç»´åº¦è¡¨ï¼Œä¾èµ–åº—é“ºæ—¥åº¦è¡¨å®Œæˆ'
            }
        ]
        
        # é¡ºåºæ‰§è¡Œï¼Œç¡®ä¿ä¾èµ–å…³ç³»
        for i, task in enumerate(etl_tasks, 1):
            logger.info(f"ğŸ“‹ ä»»åŠ¡ {i}/{len(etl_tasks)}: {task['description']}")
            execute_sql_task(task['sql'], task['name'], hints)
            
            # ä»»åŠ¡é—´çŸ­æš‚ç­‰å¾…ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            if i < len(etl_tasks):
                logger.info("â³ ç­‰å¾…æ•°æ®åŒæ­¥...")
        
        logger.info("ğŸ‰ æ‰€æœ‰ç»´åº¦è¡¨ETLä»»åŠ¡æ‰§è¡Œå®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç»´åº¦è¡¨ETLä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

# DAGé…ç½®
DAG_CONFIG = {
    'dag_id': 'dimention_maxcompute_dag',
    'description': 'MaxComputeç»´åº¦è¡¨ETLä»»åŠ¡ - åŒ…å«å•†å“å±æ€§ã€åº—é“ºæ—¥åº¦ã€å†å²äº¤æ˜“ç­‰ç»´åº¦è¡¨',
    'schedule_interval': '0 2 * * *',  # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    'start_date': datetime(2025, 1, 1),
    'catchup': False,
    'tags': ['maxcompute', 'dimension', 'etl', 'changsha'],
    'default_args': {
        'retries': 3,
        'owner': 'ChangShaTeam',
        'retry_delay': timedelta(minutes=5),
        'email_on_failure': True,
        'email_on_retry': False,
        'email': ['renruia07348@biaoguoworks.com'],
        'email_on_success': False,
        'execution_timeout': timedelta(hours=2),  # æ‰§è¡Œè¶…æ—¶æ—¶é—´
    }
}

# åˆ›å»ºDAG
with DAG(**DAG_CONFIG) as dag:
    
    # ä»»åŠ¡1: å¼€å§‹ä»»åŠ¡
    start_task = BashOperator(
        task_id='start_dimension_etl',
        bash_command='echo "ğŸš€ å¼€å§‹æ‰§è¡ŒMaxComputeç»´åº¦è¡¨ETLä»»åŠ¡ $(date)"',
        doc="æ ‡è®°ETLä»»åŠ¡å¼€å§‹"
    )
    
    # ä»»åŠ¡2: æ‰§è¡Œç»´åº¦è¡¨ETL
    dimension_etl_task = PythonOperator(
        task_id='execute_dimension_etl',
        python_callable=execute_dimension_etl,
        doc="æ‰§è¡Œç»´åº¦è¡¨ETLä»»åŠ¡ï¼ŒåŒ…æ‹¬å•†å“å±æ€§ã€åº—é“ºæ—¥åº¦ã€å†å²äº¤æ˜“ç­‰"
    )
    
    # ä»»åŠ¡3: å®Œæˆæ ‡è®°
    complete_task = BashOperator(
        task_id='complete_dimension_etl',
        bash_command='echo "âœ… MaxComputeç»´åº¦è¡¨ETLä»»åŠ¡å®Œæˆ $(date)"',
        doc="æ ‡è®°ETLä»»åŠ¡å®Œæˆ"
    )
    
    # è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
    start_task >> dimension_etl_task >> complete_task